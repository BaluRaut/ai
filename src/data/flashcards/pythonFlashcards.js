// Python & AI Flashcards

export const pythonFlashcards = {
  categories: [
    {
      id: 'python-basics',
      title: 'Python Basics',
      icon: 'üêç',
      cards: [
        {
          id: 'py-1',
          front: 'What is a list comprehension?',
          back: 'A concise way to create lists.\n\nSyntax: [expression for item in iterable if condition]\n\nExample:\nsquares = [x**2 for x in range(10)]',
          tags: ['python', 'lists', 'syntax']
        },
        {
          id: 'py-2',
          front: 'What is the difference between a list and a tuple?',
          back: '‚Ä¢ List: Mutable (can be changed), uses []\n‚Ä¢ Tuple: Immutable (cannot be changed), uses ()\n\nTuples are faster and can be used as dictionary keys.',
          tags: ['python', 'data-structures']
        },
        {
          id: 'py-3',
          front: 'What does *args and **kwargs mean?',
          back: '*args: Allows passing variable number of positional arguments (as tuple)\n\n**kwargs: Allows passing variable number of keyword arguments (as dict)\n\ndef func(*args, **kwargs):\n    print(args)   # tuple\n    print(kwargs) # dict',
          tags: ['python', 'functions']
        },
        {
          id: 'py-4',
          front: 'What is a decorator in Python?',
          back: 'A function that modifies another function\'s behavior without changing its code.\n\n@decorator\ndef my_func():\n    pass\n\nEquivalent to:\nmy_func = decorator(my_func)',
          tags: ['python', 'advanced']
        },
        {
          id: 'py-5',
          front: 'What is the difference between == and is?',
          back: '== compares values (equality)\nis compares identity (same object in memory)\n\na = [1,2]\nb = [1,2]\na == b  # True (same value)\na is b  # False (different objects)',
          tags: ['python', 'operators']
        },
        {
          id: 'py-6',
          front: 'What is a generator in Python?',
          back: 'A function that yields values one at a time, saving memory.\n\ndef count_up(n):\n    i = 0\n    while i < n:\n        yield i\n        i += 1\n\nUses yield instead of return. Creates an iterator.',
          tags: ['python', 'generators', 'memory']
        },
        {
          id: 'py-7',
          front: 'What is the GIL?',
          back: 'Global Interpreter Lock\n\nA mutex that allows only one thread to execute Python bytecode at a time.\n\nImpact: Limits true parallelism in CPU-bound multi-threaded programs.\n\nSolution: Use multiprocessing for CPU-bound tasks.',
          tags: ['python', 'threading', 'advanced']
        },
        {
          id: 'py-8',
          front: 'What are Python\'s mutable and immutable types?',
          back: 'Immutable: int, float, str, tuple, frozenset\n\nMutable: list, dict, set, custom objects\n\nImmutable objects cannot be changed after creation.',
          tags: ['python', 'data-types']
        }
      ]
    },
    {
      id: 'numpy-pandas',
      title: 'NumPy & Pandas',
      icon: 'üìä',
      cards: [
        {
          id: 'np-1',
          front: 'How do you create a NumPy array?',
          back: 'import numpy as np\n\n# From list\narr = np.array([1, 2, 3])\n\n# Zeros/Ones\nnp.zeros((3, 3))\nnp.ones((2, 4))\n\n# Range\nnp.arange(0, 10, 2)\nnp.linspace(0, 1, 5)',
          tags: ['numpy', 'arrays']
        },
        {
          id: 'np-2',
          front: 'What is broadcasting in NumPy?',
          back: 'Automatic expansion of arrays with different shapes during arithmetic.\n\nRules:\n1. Dimensions compared right-to-left\n2. Sizes must match or one must be 1\n\narr = np.array([[1,2,3]])\narr + 10  # Adds 10 to all elements',
          tags: ['numpy', 'broadcasting']
        },
        {
          id: 'np-3',
          front: 'How do you select rows in Pandas?',
          back: '# By label\ndf.loc[\'row_label\']\ndf.loc[\'a\':\'c\']  # inclusive\n\n# By position\ndf.iloc[0]\ndf.iloc[0:3]  # exclusive\n\n# Boolean\ndf[df[\'col\'] > 5]',
          tags: ['pandas', 'indexing']
        },
        {
          id: 'np-4',
          front: 'How do you handle missing values in Pandas?',
          back: '# Check for missing\ndf.isnull().sum()\n\n# Drop missing\ndf.dropna()\n\n# Fill missing\ndf.fillna(value)\ndf.fillna(df.mean())\ndf.fillna(method=\'ffill\')',
          tags: ['pandas', 'missing-data']
        },
        {
          id: 'np-5',
          front: 'What is the difference between apply, map, and applymap?',
          back: 'map: Series only, element-wise\ndf[\'col\'].map(func)\n\napply: Works on Series or DataFrame\ndf.apply(func, axis=0)  # columns\ndf.apply(func, axis=1)  # rows\n\napplymap: DataFrame only, element-wise\ndf.applymap(func)',
          tags: ['pandas', 'functions']
        },
        {
          id: 'np-6',
          front: 'How do you merge DataFrames?',
          back: '# Merge (SQL-like join)\npd.merge(df1, df2, on=\'key\')\npd.merge(df1, df2, how=\'left\')\n\n# Concat (stack)\npd.concat([df1, df2], axis=0)  # rows\npd.concat([df1, df2], axis=1)  # cols\n\n# Join (on index)\ndf1.join(df2)',
          tags: ['pandas', 'merging']
        }
      ]
    },
    {
      id: 'machine-learning',
      title: 'Machine Learning',
      icon: 'ü§ñ',
      cards: [
        {
          id: 'ml-1',
          front: 'What is the bias-variance tradeoff?',
          back: 'Bias: Error from wrong assumptions (underfitting)\nVariance: Error from sensitivity to training data (overfitting)\n\nTotal Error = Bias¬≤ + Variance + Noise\n\nGoal: Find the sweet spot that minimizes both.',
          tags: ['ml', 'fundamentals']
        },
        {
          id: 'ml-2',
          front: 'What is cross-validation?',
          back: 'Technique to evaluate model performance by splitting data into multiple folds.\n\nK-Fold: Split into K parts, train on K-1, test on 1, rotate.\n\nBenefits:\n‚Ä¢ Uses all data for both training and testing\n‚Ä¢ Reduces variance of estimate\n‚Ä¢ Detects overfitting',
          tags: ['ml', 'validation']
        },
        {
          id: 'ml-3',
          front: 'What is regularization?',
          back: 'Technique to prevent overfitting by adding penalty for complexity.\n\nL1 (Lasso): |w| penalty ‚Üí sparse weights\nL2 (Ridge): w¬≤ penalty ‚Üí small weights\nElastic Net: Both L1 + L2\n\nŒª controls regularization strength.',
          tags: ['ml', 'regularization']
        },
        {
          id: 'ml-4',
          front: 'What is gradient descent?',
          back: 'Optimization algorithm to minimize loss function.\n\nw = w - Œ± * ‚àÇL/‚àÇw\n\nVariants:\n‚Ä¢ Batch: All data per update\n‚Ä¢ Stochastic (SGD): One sample\n‚Ä¢ Mini-batch: Small batches\n\nŒ± = learning rate',
          tags: ['ml', 'optimization']
        },
        {
          id: 'ml-5',
          front: 'What is the difference between bagging and boosting?',
          back: 'Bagging: Train models in parallel on random subsets, average predictions.\nExample: Random Forest\n\nBoosting: Train models sequentially, each fixing previous errors.\nExample: XGBoost, AdaBoost\n\nBagging reduces variance.\nBoosting reduces bias.',
          tags: ['ml', 'ensemble']
        },
        {
          id: 'ml-6',
          front: 'How do you handle imbalanced classes?',
          back: '1. Resampling:\n   ‚Ä¢ Oversample minority (SMOTE)\n   ‚Ä¢ Undersample majority\n\n2. Class weights in loss function\n\n3. Different metrics:\n   ‚Ä¢ Precision, Recall, F1\n   ‚Ä¢ AUC-ROC, AUC-PR\n\n4. Threshold tuning',
          tags: ['ml', 'classification']
        },
        {
          id: 'ml-7',
          front: 'What is feature scaling and why is it important?',
          back: 'Transforming features to similar ranges.\n\nStandardization: (x - Œº) / œÉ ‚Üí mean=0, std=1\nNormalization: (x - min) / (max - min) ‚Üí [0, 1]\n\nImportant for:\n‚Ä¢ Gradient descent convergence\n‚Ä¢ Distance-based algorithms (KNN, SVM)\n‚Ä¢ Regularization fairness',
          tags: ['ml', 'preprocessing']
        },
        {
          id: 'ml-8',
          front: 'What is the curse of dimensionality?',
          back: 'Problems that arise with high-dimensional data:\n\n1. Data becomes sparse\n2. Distance metrics become meaningless\n3. More features ‚Üí need exponentially more data\n4. Overfitting more likely\n\nSolutions: PCA, feature selection, regularization',
          tags: ['ml', 'dimensionality']
        }
      ]
    },
    {
      id: 'deep-learning',
      title: 'Deep Learning',
      icon: 'üß†',
      cards: [
        {
          id: 'dl-1',
          front: 'What is backpropagation?',
          back: 'Algorithm to compute gradients in neural networks using the chain rule.\n\n1. Forward pass: compute outputs\n2. Compute loss\n3. Backward pass: compute gradients layer by layer\n4. Update weights\n\n‚àÇL/‚àÇw = ‚àÇL/‚àÇa * ‚àÇa/‚àÇz * ‚àÇz/‚àÇw',
          tags: ['dl', 'training']
        },
        {
          id: 'dl-2',
          front: 'What is the vanishing gradient problem?',
          back: 'Gradients become very small in deep networks, preventing learning in early layers.\n\nCauses: Sigmoid/tanh activation (gradients < 1)\n\nSolutions:\n‚Ä¢ ReLU activation\n‚Ä¢ Batch normalization\n‚Ä¢ Skip connections (ResNet)\n‚Ä¢ Proper initialization',
          tags: ['dl', 'training']
        },
        {
          id: 'dl-3',
          front: 'What is dropout?',
          back: 'Regularization technique that randomly sets neurons to 0 during training.\n\nnn.Dropout(p=0.5)  # 50% dropout\n\nBenefits:\n‚Ä¢ Prevents co-adaptation\n‚Ä¢ Ensemble effect\n‚Ä¢ Reduces overfitting\n\nNote: Disabled during inference.',
          tags: ['dl', 'regularization']
        },
        {
          id: 'dl-4',
          front: 'What is batch normalization?',
          back: 'Normalizes layer inputs to have mean=0, var=1.\n\nxÃÇ = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)\ny = Œ≥xÃÇ + Œ≤  # learnable params\n\nBenefits:\n‚Ä¢ Faster training\n‚Ä¢ Higher learning rates\n‚Ä¢ Reduces internal covariate shift\n‚Ä¢ Slight regularization',
          tags: ['dl', 'normalization']
        },
        {
          id: 'dl-5',
          front: 'What is the difference between CNN and RNN?',
          back: 'CNN (Convolutional Neural Network):\n‚Ä¢ Spatial patterns\n‚Ä¢ Weight sharing via filters\n‚Ä¢ Good for images\n\nRNN (Recurrent Neural Network):\n‚Ä¢ Sequential patterns\n‚Ä¢ Hidden state carries info\n‚Ä¢ Good for text, time series\n\nTransformers now often replace RNNs.',
          tags: ['dl', 'architectures']
        },
        {
          id: 'dl-6',
          front: 'What is attention mechanism?',
          back: 'Allows model to focus on relevant parts of input.\n\nAttention(Q, K, V) = softmax(QK^T / ‚àöd) * V\n\nQ: Query (what to look for)\nK: Key (what\'s available)\nV: Value (actual content)\n\nSelf-attention: Q, K, V from same input.',
          tags: ['dl', 'transformers']
        },
        {
          id: 'dl-7',
          front: 'What are common optimizers?',
          back: 'SGD: Basic, needs tuning\nSGD+Momentum: Accelerates in consistent direction\n\nAdam: Adaptive learning rates + momentum\n‚Ä¢ Most popular default\n‚Ä¢ lr=0.001 typically\n\nAdamW: Adam with proper weight decay\nRMSprop: Adaptive, good for RNNs',
          tags: ['dl', 'optimization']
        },
        {
          id: 'dl-8',
          front: 'What is transfer learning?',
          back: 'Using a pre-trained model on a new task.\n\nStrategies:\n1. Feature extraction: Freeze base, train new head\n2. Fine-tuning: Unfreeze some/all layers, train with small lr\n\nWorks because early layers learn general features (edges, textures).',
          tags: ['dl', 'transfer-learning']
        }
      ]
    },
    {
      id: 'nlp',
      title: 'NLP & LLMs',
      icon: 'üìù',
      cards: [
        {
          id: 'nlp-1',
          front: 'What is tokenization?',
          back: 'Breaking text into smaller units (tokens).\n\nTypes:\n‚Ä¢ Word: "Hello world" ‚Üí ["Hello", "world"]\n‚Ä¢ Subword (BPE): "unhappy" ‚Üí ["un", "happy"]\n‚Ä¢ Character: "Hi" ‚Üí ["H", "i"]\n\nSubword is most common for LLMs (handles OOV).',
          tags: ['nlp', 'preprocessing']
        },
        {
          id: 'nlp-2',
          front: 'What is word embedding?',
          back: 'Dense vector representation of words.\n\nWord2Vec: Predict word from context (CBOW) or context from word (Skip-gram)\n\nGloVe: Based on co-occurrence matrix\n\nProperties:\nking - man + woman ‚âà queen',
          tags: ['nlp', 'embeddings']
        },
        {
          id: 'nlp-3',
          front: 'What is a Transformer?',
          back: 'Architecture using self-attention without recurrence.\n\nComponents:\n‚Ä¢ Multi-head self-attention\n‚Ä¢ Feed-forward layers\n‚Ä¢ Positional encoding\n‚Ä¢ Layer normalization\n\nEncoder: Bidirectional (BERT)\nDecoder: Autoregressive (GPT)',
          tags: ['nlp', 'transformers']
        },
        {
          id: 'nlp-4',
          front: 'What is the difference between BERT and GPT?',
          back: 'BERT:\n‚Ä¢ Encoder-only\n‚Ä¢ Bidirectional context\n‚Ä¢ Good for understanding\n‚Ä¢ Tasks: classification, NER, QA\n\nGPT:\n‚Ä¢ Decoder-only\n‚Ä¢ Autoregressive (left-to-right)\n‚Ä¢ Good for generation\n‚Ä¢ Tasks: text generation, chat',
          tags: ['nlp', 'llm']
        },
        {
          id: 'nlp-5',
          front: 'What is prompt engineering?',
          back: 'Designing inputs to get desired outputs from LLMs.\n\nTechniques:\n‚Ä¢ Zero-shot: Direct question\n‚Ä¢ Few-shot: Include examples\n‚Ä¢ Chain-of-thought: "Let\'s think step by step"\n‚Ä¢ System prompts: Set behavior\n\nBe specific, provide context, iterate.',
          tags: ['nlp', 'llm', 'prompting']
        },
        {
          id: 'nlp-6',
          front: 'What is RAG (Retrieval Augmented Generation)?',
          back: 'Combining retrieval with generation.\n\n1. User query ‚Üí embed\n2. Search vector DB for relevant docs\n3. Add docs to prompt context\n4. LLM generates answer using context\n\nBenefits: Reduces hallucination, uses current info.',
          tags: ['nlp', 'llm', 'rag']
        },
        {
          id: 'nlp-7',
          front: 'What is fine-tuning vs prompting?',
          back: 'Prompting:\n‚Ä¢ No training, instant\n‚Ä¢ Limited by context window\n‚Ä¢ Flexible, easy to change\n\nFine-tuning:\n‚Ä¢ Requires training data\n‚Ä¢ Permanent behavior change\n‚Ä¢ Better for specific tasks\n‚Ä¢ More expensive\n\nTry prompting first!',
          tags: ['nlp', 'llm']
        }
      ]
    },
    {
      id: 'statistics',
      title: 'Statistics & Probability',
      icon: 'üìà',
      cards: [
        {
          id: 'stat-1',
          front: 'What is p-value?',
          back: 'Probability of observing results at least as extreme as the data, assuming null hypothesis is true.\n\np < 0.05: Reject null (statistically significant)\np ‚â• 0.05: Fail to reject null\n\nNote: p-value ‚â† probability that null is true!',
          tags: ['statistics', 'hypothesis-testing']
        },
        {
          id: 'stat-2',
          front: 'What is Bayes\' Theorem?',
          back: 'P(A|B) = P(B|A) * P(A) / P(B)\n\nPosterior = Likelihood √ó Prior / Evidence\n\nUpdates probability based on new evidence.\n\nExample: Disease testing - probability of disease given positive test.',
          tags: ['statistics', 'probability']
        },
        {
          id: 'stat-3',
          front: 'What is the Central Limit Theorem?',
          back: 'Sample means approach normal distribution as sample size increases, regardless of population distribution.\n\nMean of sample means = population mean\nStd of sample means = œÉ/‚àön\n\nRequires n ‚â• 30 typically.',
          tags: ['statistics', 'distributions']
        },
        {
          id: 'stat-4',
          front: 'What is Type I vs Type II error?',
          back: 'Type I (False Positive): Rejecting true null\n‚Ä¢ Probability = Œ± (significance level)\n‚Ä¢ "Crying wolf"\n\nType II (False Negative): Not rejecting false null\n‚Ä¢ Probability = Œ≤\n‚Ä¢ Power = 1 - Œ≤\n‚Ä¢ "Missing the wolf"',
          tags: ['statistics', 'hypothesis-testing']
        },
        {
          id: 'stat-5',
          front: 'What is correlation vs causation?',
          back: 'Correlation: Variables move together\n‚Ä¢ r = -1 to 1\n‚Ä¢ Does NOT imply causation\n\nCausation: One variable causes change in another\n‚Ä¢ Requires: correlation + time order + no confounders\n‚Ä¢ Best proven by randomized experiments',
          tags: ['statistics', 'fundamentals']
        },
        {
          id: 'stat-6',
          front: 'What is A/B testing?',
          back: 'Randomized controlled experiment comparing two variants.\n\nSteps:\n1. Define hypothesis & metric\n2. Calculate sample size needed\n3. Randomly assign users\n4. Run experiment\n5. Analyze with statistical test\n\nWatch for: novelty effect, multiple testing.',
          tags: ['statistics', 'experimentation']
        }
      ]
    }
  ]
};

export default pythonFlashcards;
