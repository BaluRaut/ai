export const aiQuizBankExpansion10 = [
  {
    id: "rl-genai-001",
    question: "In Q-Learning, what does the 'Q' value represent?",
    options: [
      "The quality of an action in a given state",
      "The quantity of rewards received",
      "The quickness of the agent's response",
      "The quotient of the learning rate"
    ],
    correctAnswer: 0,
    explanation: "The Q-value (Quality) represents the expected cumulative future reward of taking a specific action in a specific state and following the optimal policy thereafter.",
    difficulty: "easy",
    tags: ["Reinforcement Learning", "Q-Learning"]
  },
  {
    id: "rl-genai-002",
    question: "Which equation is fundamental to the update rule in Q-Learning?",
    options: [
      "Maxwell's Equation",
      "Bellman Equation",
      "Schr√∂dinger Equation",
      "Euler's Identity"
    ],
    correctAnswer: 1,
    explanation: "The Bellman Equation is used to update Q-values by relating the value of the current state-action pair to the reward and the value of the next state.",
    difficulty: "easy",
    tags: ["Reinforcement Learning", "Q-Learning"]
  },
  {
    id: "rl-genai-003",
    question: "What is the purpose of the epsilon-greedy strategy in Reinforcement Learning?",
    options: [
      "To always choose the best action",
      "To balance exploration and exploitation",
      "To minimize the learning rate",
      "To calculate the discount factor"
    ],
    correctAnswer: 1,
    explanation: "Epsilon-greedy is a policy that balances exploration (choosing random actions) and exploitation (choosing the best known action) to ensure the agent learns the environment effectively.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "Q-Learning"]
  },
  {
    id: "rl-genai-004",
    question: "Q-Learning is considered an 'off-policy' algorithm. What does this mean?",
    options: [
      "It learns from data generated by a different policy than the one it is improving",
      "It requires the agent to be offline to learn",
      "It does not use a policy at all",
      "It only learns when the policy is optimal"
    ],
    correctAnswer: 0,
    explanation: "Off-policy learning means the agent learns the value of the optimal policy independently of the agent's actions (which might be exploratory/random).",
    difficulty: "hard",
    tags: ["Reinforcement Learning", "Q-Learning"]
  },
  {
    id: "rl-genai-005",
    question: "In Deep Q-Networks (DQN), what is the purpose of 'Experience Replay'?",
    options: [
      "To replay the game for the user",
      "To break correlations between consecutive samples",
      "To increase the frame rate of the simulation",
      "To reduce the size of the neural network"
    ],
    correctAnswer: 1,
    explanation: "Experience Replay stores past transitions in a buffer and samples them randomly during training, breaking the temporal correlation between consecutive steps and stabilizing training.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "Deep Learning"]
  },
  {
    id: "rl-genai-006",
    question: "What is the main difference between Policy Gradient methods and Q-Learning?",
    options: [
      "Policy Gradients optimize the policy directly; Q-Learning optimizes a value function",
      "Policy Gradients are always faster",
      "Q-Learning cannot handle discrete action spaces",
      "Policy Gradients do not use rewards"
    ],
    correctAnswer: 0,
    explanation: "Policy Gradient methods parameterize the policy and optimize it directly via gradient ascent, whereas Q-Learning learns a value function (Q-values) to derive a policy.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "Policy Gradients"]
  },
  {
    id: "rl-genai-007",
    question: "Which algorithm is a classic example of a Policy Gradient method?",
    options: [
      "Dijkstra's Algorithm",
      "REINFORCE",
      "K-Means Clustering",
      "Random Forest"
    ],
    correctAnswer: 1,
    explanation: "REINFORCE is a fundamental Monte Carlo policy gradient algorithm that uses full trajectory returns to update policy parameters.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "Policy Gradients"]
  },
  {
    id: "rl-genai-008",
    question: "In Actor-Critic methods, what does the 'Critic' do?",
    options: [
      "It selects the actions",
      "It evaluates the action taken by the Actor by estimating the value function",
      "It criticizes the reward function",
      "It stops the training early"
    ],
    correctAnswer: 1,
    explanation: "The Critic estimates the value function (how good a state or action is), which helps reduce the variance of the gradient updates for the Actor (which selects actions).",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "Actor-Critic"]
  },
  {
    id: "rl-genai-009",
    question: "What is the 'Advantage Function' in Reinforcement Learning?",
    options: [
      "The benefit of using a GPU over a CPU",
      "A measure of how much better an action is compared to the average action in that state",
      "The total reward accumulated so far",
      "The probability of winning the game"
    ],
    correctAnswer: 1,
    explanation: "The Advantage Function A(s,a) = Q(s,a) - V(s) represents the relative value of a specific action compared to the average value of the state.",
    difficulty: "hard",
    tags: ["Reinforcement Learning", "Policy Gradients"]
  },
  {
    id: "rl-genai-010",
    question: "Proximal Policy Optimization (PPO) is popular because:",
    options: [
      "It is the oldest RL algorithm",
      "It strikes a balance between ease of implementation, sample efficiency, and tuning",
      "It guarantees finding the global optimum every time",
      "It does not require a reward function"
    ],
    correctAnswer: 1,
    explanation: "PPO is widely used because it prevents large, destructive policy updates (via clipping) while being simpler to implement than TRPO and reasonably sample efficient.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "PPO"]
  },
  {
    id: "rl-genai-011",
    question: "What does RLHF stand for?",
    options: [
      "Reinforcement Learning with Heuristic Functions",
      "Reinforcement Learning from Human Feedback",
      "Recursive Learning for High Fidelity",
      "Randomized Learning in Hidden Fields"
    ],
    correctAnswer: 1,
    explanation: "RLHF stands for Reinforcement Learning from Human Feedback, a technique used to align AI models with human values and preferences.",
    difficulty: "easy",
    tags: ["Reinforcement Learning", "RLHF"]
  },
  {
    id: "rl-genai-012",
    question: "In the standard RLHF pipeline for LLMs, what is trained immediately after Supervised Fine-Tuning (SFT)?",
    options: [
      "The final policy",
      "A Reward Model (RM)",
      "A Vision Transformer",
      "A Q-Table"
    ],
    correctAnswer: 1,
    explanation: "After SFT, a Reward Model is trained on a dataset of human comparisons (rankings of model outputs) to predict which response a human would prefer.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "RLHF"]
  },
  {
    id: "rl-genai-013",
    question: "Why is a KL Divergence penalty often added to the reward in RLHF?",
    options: [
      "To increase the model's creativity",
      "To prevent the model from drifting too far from the original supervised model",
      "To speed up the training process",
      "To reduce the memory usage"
    ],
    correctAnswer: 1,
    explanation: "The KL penalty ensures the RL-tuned model doesn't deviate excessively from the initial SFT model, preventing 'reward hacking' where the model generates gibberish that satisfies the reward model.",
    difficulty: "hard",
    tags: ["Reinforcement Learning", "RLHF"]
  },
  {
    id: "rl-genai-014",
    question: "Which algorithm is commonly used for the reinforcement learning phase in RLHF for LLMs?",
    options: [
      "DQN",
      "PPO (Proximal Policy Optimization)",
      "Genetic Algorithms",
      "Simulated Annealing"
    ],
    correctAnswer: 1,
    explanation: "PPO is the standard choice for the RL step in RLHF due to its stability and ability to handle the continuous updates required for language model tuning.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "RLHF"]
  },
  {
    id: "rl-genai-015",
    question: "What is the primary goal of RLHF in the context of Large Language Models?",
    options: [
      "To make the model generate longer text",
      "To align the model's outputs with human intent (helpfulness, safety, honesty)",
      "To reduce the number of parameters in the model",
      "To enable the model to process images"
    ],
    correctAnswer: 1,
    explanation: "RLHF is specifically designed to align the model with complex human values that are difficult to specify via a simple loss function.",
    difficulty: "easy",
    tags: ["Reinforcement Learning", "RLHF"]
  },
  {
    id: "rl-genai-016",
    question: "What is 'Reward Hacking' in RL?",
    options: [
      "Stealing rewards from other agents",
      "The agent finding a loophole to maximize reward without achieving the intended goal",
      "Manually editing the reward function code",
      "Getting negative rewards"
    ],
    correctAnswer: 1,
    explanation: "Reward hacking occurs when an agent exploits flaws in the reward function design to get high scores in unintended or useless ways.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "Safety"]
  },
  {
    id: "rl-genai-017",
    question: "In RLHF, how is the training data for the Reward Model typically collected?",
    options: [
      "By letting the model play video games",
      "By having humans rank or compare multiple model outputs for the same prompt",
      "By scraping Wikipedia",
      "By using a random number generator"
    ],
    correctAnswer: 1,
    explanation: "Human annotators are shown multiple responses to a prompt and asked to rank them from best to worst. This ranking data is used to train the Reward Model.",
    difficulty: "medium",
    tags: ["Reinforcement Learning", "RLHF"]
  },
  {
    id: "rl-genai-018",
    question: "What is 'Constitutional AI'?",
    options: [
      "AI that runs the government",
      "A method where AI helps supervise other AI using a set of principles (constitution)",
      "AI trained only on legal documents",
      "A deprecated version of RLHF"
    ],
    correctAnswer: 1,
    explanation: "Constitutional AI involves using a set of high-level principles (a constitution) to guide the AI's behavior, often using AI feedback (RLAIF) instead of just human feedback.",
    difficulty: "hard",
    tags: ["Reinforcement Learning", "GenAI"]
  },
  {
    id: "rl-genai-019",
    question: "Which component is NOT part of the standard Transformer architecture?",
    options: [
      "Self-Attention Mechanism",
      "Feed-Forward Networks",
      "Convolutional Layers",
      "Positional Encodings"
    ],
    correctAnswer: 2,
    explanation: "Standard Transformers rely on attention mechanisms and feed-forward layers. Convolutions are characteristic of CNNs, though some hybrid architectures exist.",
    difficulty: "easy",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-020",
    question: "What problem does the 'Self-Attention' mechanism solve?",
    options: [
      "It reduces the image resolution",
      "It allows the model to weigh the importance of different words in a sequence relative to each other",
      "It prevents overfitting",
      "It converts text to speech"
    ],
    correctAnswer: 1,
    explanation: "Self-attention enables the model to look at other words in the input sentence to better understand the context of the current word.",
    difficulty: "medium",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-021",
    question: "Why are Positional Encodings necessary in Transformers?",
    options: [
      "Because the attention mechanism is permutation invariant (has no inherent notion of order)",
      "To encrypt the data",
      "To compress the input sequence",
      "They are not necessary"
    ],
    correctAnswer: 0,
    explanation: "Unlike RNNs, Transformers process tokens in parallel. Positional encodings are added to embeddings to give the model information about the order of words.",
    difficulty: "medium",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-022",
    question: "What is the core idea behind 'Multi-Head Attention'?",
    options: [
      "Using multiple brains to train the model",
      "Running attention multiple times in parallel to capture different types of relationships",
      "Attending to multiple documents at once",
      "Using multiple GPUs"
    ],
    correctAnswer: 1,
    explanation: "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.",
    difficulty: "medium",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-023",
    question: "BERT is an example of which part of the Transformer architecture?",
    options: [
      "Encoder-only",
      "Decoder-only",
      "Encoder-Decoder",
      "Recurrent unit"
    ],
    correctAnswer: 0,
    explanation: "BERT (Bidirectional Encoder Representations from Transformers) utilizes the Encoder stack to create rich contextual embeddings.",
    difficulty: "medium",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-024",
    question: "GPT (Generative Pre-trained Transformer) is an example of:",
    options: [
      "Encoder-only",
      "Decoder-only",
      "Encoder-Decoder",
      "CNN"
    ],
    correctAnswer: 1,
    explanation: "GPT models use the Decoder stack of the Transformer, utilizing masked self-attention to generate text autoregressively (predicting the next token).",
    difficulty: "medium",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-025",
    question: "What is the 'Context Window' in an LLM?",
    options: [
      "The GUI of the application",
      "The maximum amount of text (tokens) the model can consider at one time",
      "The time it takes to train the model",
      "The hidden layer size"
    ],
    correctAnswer: 1,
    explanation: "The context window limits how much previous text (prompt + generation) the model can 'see' and attend to when generating the next token.",
    difficulty: "easy",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-026",
    question: "What is 'Temperature' in the context of LLM sampling?",
    options: [
      "The physical heat of the GPU",
      "A parameter that controls the randomness of the output",
      "The learning rate schedule",
      "The bias of the model"
    ],
    correctAnswer: 1,
    explanation: "Temperature scales the logits before softmax. High temperature increases diversity (randomness), while low temperature makes the output more deterministic.",
    difficulty: "easy",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-027",
    question: "What is 'Chain-of-Thought' prompting?",
    options: [
      "Linking multiple GPUs together",
      "Asking the model to explain its reasoning step-by-step before giving the final answer",
      "A type of blockchain for AI",
      "Prompting the model with a chain of random words"
    ],
    correctAnswer: 1,
    explanation: "Chain-of-Thought prompting encourages the model to decompose complex problems into intermediate reasoning steps, significantly improving performance on logic and math tasks.",
    difficulty: "medium",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-028",
    question: "What is a 'Hallucination' in Generative AI?",
    options: [
      "When the model sees ghosts",
      "When the model generates factually incorrect or nonsensical information confidently",
      "When the model crashes",
      "When the model refuses to answer"
    ],
    correctAnswer: 1,
    explanation: "Hallucination refers to the generation of content that looks plausible but is factually incorrect or unrelated to the source input.",
    difficulty: "easy",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-029",
    question: "What is the fundamental principle of Diffusion Models?",
    options: [
      "Adversarial training",
      "Gradually adding noise to data and then learning to reverse the process",
      "Maximizing the Q-value",
      "Sorting data efficiently"
    ],
    correctAnswer: 1,
    explanation: "Diffusion models work by destroying training data through the successive addition of Gaussian noise, and then learning to recover the data by reversing this noise process.",
    difficulty: "medium",
    tags: ["Generative AI", "Diffusion Models"]
  },
  {
    id: "rl-genai-030",
    question: "In Stable Diffusion, what is the 'Latent Space'?",
    options: [
      "The space between the keyboard and chair",
      "A compressed representation of the image where the diffusion process happens",
      "The hard drive storage",
      "The internet"
    ],
    correctAnswer: 1,
    explanation: "Stable Diffusion (Latent Diffusion) operates in a lower-dimensional latent space rather than pixel space, making it much more computationally efficient.",
    difficulty: "medium",
    tags: ["Generative AI", "Diffusion Models"]
  },
  {
    id: "rl-genai-031",
    question: "What architecture is commonly used as the noise predictor in Diffusion Models?",
    options: [
      "U-Net",
      "ResNet-50",
      "LSTM",
      "Decision Tree"
    ],
    correctAnswer: 0,
    explanation: "The U-Net architecture, often with attention mechanisms, is the standard choice for predicting the noise residual at each step of the reverse diffusion process.",
    difficulty: "hard",
    tags: ["Generative AI", "Diffusion Models"]
  },
  {
    id: "rl-genai-032",
    question: "How does text-to-image conditioning work in Diffusion Models?",
    options: [
      "The text is converted to an image directly",
      "Text embeddings (e.g., from CLIP) are injected into the U-Net via cross-attention",
      "The text is ignored",
      "The text is used as the file name"
    ],
    correctAnswer: 1,
    explanation: "Text prompts are encoded into vectors (using models like CLIP) and interact with the image generation process through cross-attention layers in the U-Net.",
    difficulty: "hard",
    tags: ["Generative AI", "Diffusion Models"]
  },
  {
    id: "rl-genai-033",
    question: "What is LoRA (Low-Rank Adaptation)?",
    options: [
      "A new programming language",
      "A parameter-efficient fine-tuning technique that freezes pre-trained weights and injects trainable rank decomposition matrices",
      "A type of reinforcement learning",
      "A database for AI"
    ],
    correctAnswer: 1,
    explanation: "LoRA allows fine-tuning of massive models by training only a small number of extra parameters (low-rank matrices), drastically reducing memory requirements.",
    difficulty: "hard",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-034",
    question: "What is 'Zero-shot Learning'?",
    options: [
      "Learning without any training data at all",
      "The ability of a model to perform a task without seeing any specific examples of that task in the prompt",
      "Learning with zero errors",
      "Learning in zero seconds"
    ],
    correctAnswer: 1,
    explanation: "Zero-shot learning refers to a model's ability to handle unseen tasks or classes relying solely on its pre-training knowledge and the task description.",
    difficulty: "medium",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-035",
    question: "What is the 'Vanishing Gradient' problem, and how do Transformers mitigate it?",
    options: [
      "Gradients disappear in the mail; Transformers use email",
      "Gradients become too small for learning; Transformers use residual connections and layer normalization",
      "Gradients explode; Transformers use water cooling",
      "It is not a problem in Deep Learning"
    ],
    correctAnswer: 1,
    explanation: "Residual (skip) connections allow gradients to flow through the network more easily, mitigating the vanishing gradient problem common in deep networks.",
    difficulty: "medium",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-036",
    question: "What is 'Tokenization'?",
    options: [
      "Buying crypto tokens",
      "Converting text into smaller units (words, subwords, or characters) for the model to process",
      "Securing the API",
      "Gamification of learning"
    ],
    correctAnswer: 1,
    explanation: "Tokenization is the process of breaking down text into numerical tokens (IDs) that the neural network can process.",
    difficulty: "easy",
    tags: ["Generative AI", "NLP"]
  },
  {
    id: "rl-genai-037",
    question: "Which of these is a key advantage of Diffusion Models over GANs?",
    options: [
      "Faster inference speed",
      "Training stability and mode coverage",
      "Simpler architecture",
      "Requires less data"
    ],
    correctAnswer: 1,
    explanation: "GANs are notoriously difficult to train (mode collapse, instability). Diffusion models offer stable training objectives and better coverage of the data distribution.",
    difficulty: "medium",
    tags: ["Generative AI", "Diffusion Models"]
  },
  {
    id: "rl-genai-038",
    question: "What is 'RAG' (Retrieval-Augmented Generation)?",
    options: [
      "A cleaning cloth for servers",
      "Combining an LLM with an external knowledge retrieval system to improve accuracy",
      "Randomly Augmented Generation",
      "Recursive Algorithm Generation"
    ],
    correctAnswer: 1,
    explanation: "RAG retrieves relevant documents from an external database and provides them as context to the LLM, helping to reduce hallucinations and provide up-to-date information.",
    difficulty: "medium",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-039",
    question: "What is the 'Attention Mask' used for?",
    options: [
      "To hide the identity of the user",
      "To prevent the model from attending to padding tokens or future tokens (in causal masking)",
      "To focus only on the first word",
      "To mask the output image"
    ],
    correctAnswer: 1,
    explanation: "Attention masks ensure the model doesn't process irrelevant parts of the input (like padding) or cheat by looking at future words during training (causal masking).",
    difficulty: "medium",
    tags: ["Generative AI", "Transformers"]
  },
  {
    id: "rl-genai-040",
    question: "In the context of LLMs, what are 'Scaling Laws'?",
    options: [
      "Laws regulating the weight of the hardware",
      "Empirical observations that performance improves predictably with model size, data size, and compute",
      "Legal restrictions on AI growth",
      "Rules for scaling images"
    ],
    correctAnswer: 1,
    explanation: "Scaling laws (e.g., Kaplan et al., Chinchilla) describe the power-law relationship between model performance (loss) and the scale of compute, data, and parameters.",
    difficulty: "hard",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-041",
    question: "What is 'Beam Search'?",
    options: [
      "Searching with a flashlight",
      "A decoding algorithm that keeps track of the top-k most probable sequences",
      "A laser-based data transfer",
      "Searching for structural beams"
    ],
    correctAnswer: 1,
    explanation: "Beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set, used to generate better text than greedy decoding.",
    difficulty: "medium",
    tags: ["Generative AI", "NLP"]
  },
  {
    id: "rl-genai-042",
    question: "What is the role of the 'Discriminator' in a GAN (Generative Adversarial Network)?",
    options: [
      "To generate the images",
      "To distinguish between real and fake data",
      "To discriminate against bad users",
      "To optimize the learning rate"
    ],
    correctAnswer: 1,
    explanation: "In a GAN, the Discriminator tries to classify inputs as real (from the dataset) or fake (from the Generator), forcing the Generator to improve.",
    difficulty: "easy",
    tags: ["Generative AI", "GANs"]
  },
  {
    id: "rl-genai-043",
    question: "What is 'Mode Collapse' in generative models?",
    options: [
      "When the model falls asleep",
      "When the model produces only a limited variety of outputs (e.g., the same face over and over)",
      "When the loss function reaches zero",
      "When the model runs out of memory"
    ],
    correctAnswer: 1,
    explanation: "Mode collapse happens when a generative model (typically a GAN) finds a few outputs that fool the discriminator and produces only those, ignoring the diversity of the real data.",
    difficulty: "medium",
    tags: ["Generative AI", "GANs"]
  },
  {
    id: "rl-genai-044",
    question: "What is 'Prompt Engineering'?",
    options: [
      "Building physical bridges quickly",
      "The art of crafting inputs (prompts) to guide an LLM to generate the desired output",
      "Engineering the model architecture",
      "Writing code in Python"
    ],
    correctAnswer: 1,
    explanation: "Prompt engineering involves designing and refining the text input to an LLM to elicit the best possible response for a specific task.",
    difficulty: "easy",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-045",
    question: "What is the difference between 'Fine-Tuning' and 'Pre-training'?",
    options: [
      "Pre-training is on a small dataset; Fine-tuning is on a large one",
      "Pre-training teaches general language patterns on massive data; Fine-tuning adapts the model to a specific task",
      "They are the same thing",
      "Fine-tuning happens before Pre-training"
    ],
    correctAnswer: 1,
    explanation: "Pre-training is the computationally expensive phase of learning general knowledge from vast corpora. Fine-tuning is the subsequent phase of specializing the model for a use case.",
    difficulty: "easy",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-046",
    question: "What is 'Classifier-Free Guidance' in diffusion models?",
    options: [
      "Guiding the model without a separate classifier network",
      "Removing all classification tasks",
      "Free guidance for everyone",
      "Using a random classifier"
    ],
    correctAnswer: 0,
    explanation: "Classifier-Free Guidance improves sample quality and adherence to the prompt by interpolating between the predictions of a conditional model and an unconditional model, without needing an external classifier.",
    difficulty: "hard",
    tags: ["Generative AI", "Diffusion Models"]
  },
  {
    id: "rl-genai-047",
    question: "What is 'Perplexity' in NLP?",
    options: [
      "How confused the user is",
      "A measurement of how well a probability model predicts a sample",
      "The complexity of the code",
      "The number of parameters"
    ],
    correctAnswer: 1,
    explanation: "Perplexity is a common metric for evaluating language models. Lower perplexity indicates the model is less 'surprised' by the text and predicts it better.",
    difficulty: "medium",
    tags: ["Generative AI", "NLP"]
  },
  {
    id: "rl-genai-048",
    question: "What is the 'KV Cache' (Key-Value Cache) used for in LLM inference?",
    options: [
      "To store user passwords",
      "To speed up generation by caching attention keys and values for previous tokens",
      "To cache the model weights",
      "To store the training data"
    ],
    correctAnswer: 1,
    explanation: "KV Cache stores the Key and Value matrices for tokens already processed, so they don't need to be recomputed at every step of autoregressive generation.",
    difficulty: "hard",
    tags: ["Generative AI", "LLMs"]
  },
  {
    id: "rl-genai-049",
    question: "What is 'DPO' (Direct Preference Optimization)?",
    options: [
      "Data Processing Office",
      "A method to align LLMs using preference data without an explicit reward model or RL loop",
      "Direct Policy Output",
      "Digital Preference Option"
    ],
    correctAnswer: 1,
    explanation: "DPO optimizes the policy directly on preference data by implicitly defining the reward function, offering a more stable and simpler alternative to PPO-based RLHF.",
    difficulty: "hard",
    tags: ["Generative AI", "RLHF"]
  },
  {
    id: "rl-genai-050",
    question: "What is 'Emergence' in Large Language Models?",
    options: [
      "The model coming out of the screen",
      "Capabilities that appear suddenly in larger models but are not present in smaller ones",
      "The release date of the model",
      "Emergency shutdown protocols"
    ],
    correctAnswer: 1,
    explanation: "Emergent abilities (e.g., arithmetic, few-shot learning) are capabilities that are not observed in smaller models but arise unpredictably as the model scale increases.",
    difficulty: "medium",
    tags: ["Generative AI", "LLMs"]
  }
];